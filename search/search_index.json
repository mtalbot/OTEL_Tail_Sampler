{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"OTEL Tail Sampler: Intelligent Observability at Scale","text":"<p>Developed by MarsLabs</p> <p>The High-Performance, Distributed Solution for Tail Sampling and Cardinality Control.</p> <p>In modern distributed systems, data volume is your greatest asset and your biggest liability. Traditional sampling drops the very traces you need most\u2014the errors, the outliers, and the bottlenecks. </p> <p>OTEL Tail Sampler changes the game. By buffering data and coordinating decisions across your entire cluster, we ensure you keep the 1% of data that matters, while intelligently rolling up the 99% that doesn't.</p>"},{"location":"#key-benefits","title":"\ud83d\ude80 Key Benefits","text":""},{"location":"#never-miss-a-performance-outlier","title":"\ud83c\udfaf Never Miss a Performance Outlier","text":"<p>Our distributed tail sampling ensures that if a trace is interesting (errors, high latency, or specific business attributes), the entire trace is captured across all nodes, along with its associated logs and fine-grained metrics.</p>"},{"location":"#massive-cost-reduction","title":"\ud83d\udcb0 Massive Cost Reduction","text":"<p>Stop paying for redundant data. Our dynamic rollup processor aggregates unsampled metrics and logs into high-level summaries, providing 90%+ data reduction while maintaining total visibility.</p>"},{"location":"#enterprise-grade-reliability","title":"\ud83d\udee1\ufe0f Enterprise-Grade Reliability","text":"<p>With a high-performance Write-Ahead Log (WAL) and BigCache-backed in-memory storage, your data is safe from crashes and your service stays fast with zero garbage collection overhead.</p>"},{"location":"#features-for-engineers","title":"\ud83d\udee0 Features for Engineers","text":"<ul> <li>Protobuf-Powered Gossip: Low-latency cluster coordination without a central coordinator.</li> <li>Zero-GC Architecture: Uses BigCache for high-throughput buffering without performance degradation.</li> <li>Contextual Correlation: Automatically links logs and metrics to sampled traces within the same time window.</li> <li>OTLP Native: Built on OpenTelemetry standards for seamless integration with your existing stack.</li> </ul>"},{"location":"#get-started","title":"\ud83c\udfc1 Get Started","text":"<p>Ready to optimize your observability pipeline?</p> <p>View Getting Started Guide | Technical Architecture | Configuration Reference</p>"},{"location":"architecture/overview/","title":"Technical Architecture","text":"<p>The OTEL Tail Sampler is designed for high throughput and high availability. It operates as a middle-tier service between your applications (or an initial OTel Collector) and your long-term storage backend.</p>"},{"location":"architecture/overview/#data-flow","title":"Data Flow","text":"<ol> <li>Ingestion: Data enters via gRPC OTLP.</li> <li>Durability (WAL): Every signal is immediately written to a high-speed Write-Ahead Log on disk.</li> <li>Buffering: Data is stored in BigCache, an off-heap in-memory store that avoids Go Garbage Collection overhead.</li> <li>Evaluation: The Decision Engine evaluates traces against configured policies.</li> <li>Coordination: If a node decides to sample, it broadcasts a Protobuf-encoded decision via the Gossip Protocol.</li> <li>Processing: All nodes in the cluster receive the decision and flush matching data from their local buffers.</li> <li>Rollup: Unsampled data is periodically aggregated by the Rollup Processor into high-level summaries.</li> <li>Export: Flagged data is exported to the final OTLP destination.</li> </ol>"},{"location":"architecture/overview/#key-components","title":"Key Components","text":""},{"location":"architecture/overview/#gossip-protocol-memberlist","title":"Gossip Protocol (<code>memberlist</code>)","text":"<p>We use a SWIM-based gossip protocol to maintain cluster membership and propagate sampling decisions. This removes the need for a central database or coordinator, ensuring the sampler can scale horizontally with ease.</p>"},{"location":"architecture/overview/#bigcache-buffer","title":"BigCache Buffer","text":"<p>To handle millions of spans per second, we utilize BigCache. By serializing OTLP data into byte arrays and storing them off-heap, we maintain sub-millisecond response times even under heavy load, with zero GC pauses.</p>"},{"location":"architecture/overview/#decision-engine","title":"Decision Engine","text":"<p>The engine supports multiple concurrent policies: *   Latency: Sample if any span in a trace exceeds a threshold. *   Error: Sample if any span contains an error status. *   Attribute: Sample based on specific tags (e.g., <code>user_tier == \"gold\"</code>).</p>"},{"location":"architecture/overview/#rollup-processor","title":"Rollup Processor","text":"<p>A unique feature that ensures visibility even for unsampled data. It reduces cardinality by: *   Aggregating metrics by name and common attributes. *   Grouping logs by severity and body patterns (prefix matching).</p>"},{"location":"configuration/reference/","title":"Configuration Reference","text":"<p>The sampler is configured via a YAML file or environment variables.</p>"},{"location":"configuration/reference/#top-level-options","title":"Top-Level Options","text":"Option Description Default <code>debug</code> Enable verbose debug logging. <code>true</code>"},{"location":"configuration/reference/#receiver-settings-receiver","title":"Receiver Settings (<code>receiver</code>)","text":"Option Description Default <code>grpc_port</code> Port for OTLP gRPC ingestion. <code>4317</code> <code>host</code> Host address to bind to. <code>0.0.0.0</code>"},{"location":"configuration/reference/#buffer-settings-buffer","title":"Buffer Settings (<code>buffer</code>)","text":"Option Description Default <code>size</code> Maximum cache size in MB. <code>512</code> <code>ttl_seconds</code> How long to buffer data before decision or rollup. <code>30</code> <code>max_traces</code> Maximum number of trace entries to index. <code>5000</code>"},{"location":"configuration/reference/#gossip-settings-gossip","title":"Gossip Settings (<code>gossip</code>)","text":"Option Description Default <code>port</code> Port for cluster coordination. <code>7946</code> <code>bind_addr</code> IP to bind for gossip. <code>0.0.0.0</code> <code>interval</code> Gossip message interval in ms. <code>100</code>"},{"location":"configuration/reference/#decision-policies-decisionpolicies","title":"Decision Policies (<code>decision.policies</code>)","text":"<p>Define a list of triggers.</p> <pre><code>decision:\n  policies:\n    - name: \"slow-traces\"\n      type: \"latency\"\n      latency_threshold_ms: 500\n    - name: \"errors\"\n      type: \"error\"\n    - name: \"vip-customers\"\n      type: \"attribute\"\n      attribute_key: \"customer.type\"\n      attribute_value: \"enterprise\"\n</code></pre>"},{"location":"configuration/reference/#exporter-settings-exporter","title":"Exporter Settings (<code>exporter</code>)","text":"Option Description Default <code>endpoint</code> Downstream OTLP gRPC endpoint. <code>localhost:4317</code> <code>insecure</code> Use insecure connection. <code>true</code>"},{"location":"getting-started/quickstart/","title":"Quickstart Guide","text":"<p>Get the OTEL Tail Sampler up and running in less than 5 minutes.</p>"},{"location":"getting-started/quickstart/#1-prerequisites","title":"1. Prerequisites","text":"<ul> <li>Docker installed</li> <li>Kubernetes cluster (optional, for Helm)</li> <li><code>helm</code> (optional)</li> </ul>"},{"location":"getting-started/quickstart/#2-running-with-docker","title":"2. Running with Docker","text":"<p>The easiest way to try the sampler is using our pre-built image.</p> <pre><code>docker run -p 4317:4317 -p 7946:7946 \\\n  -e EXPORTER_ENDPOINT=\"your-backend:4317\" \\\n  otel-sampler:latest\n</code></pre>"},{"location":"getting-started/quickstart/#3-deploying-with-helm","title":"3. Deploying with Helm","text":"<p>For production-like environments, use the provided Helm chart.</p> <pre><code># Clone the repo\ngit clone https://github.com/mtalbot/OTEL_Tail_Sampler.git\ncd OTEL_Tail_Sampler\n\n# Install the chart\nhelm install sampler deploy/helm/otel-tail-sampler \\\n  --set config.exporter.endpoint=\"otel-collector:4317\"\n</code></pre>"},{"location":"getting-started/quickstart/#4-verify-data-flow","title":"4. Verify Data Flow","text":"<p>Send a test trace using <code>telemetrygen</code>:</p> <pre><code>docker run ghcr.io/open-telemetry/opentelemetry-collector-contrib/telemetrygen:latest \\\n  traces --otlp-endpoint=localhost:4317 --otlp-insecure --rate=1 --duration=5s\n</code></pre> <p>Check the logs: <pre><code># If running in Docker\ndocker logs &lt;container_id&gt; | grep \"Sampling trace\"\n</code></pre></p>"},{"location":"getting-started/quickstart/#5-next-steps","title":"5. Next Steps","text":"<ul> <li>Explore Configuration Reference to define your sampling policies.</li> <li>Learn about the Technical Architecture.</li> </ul>"},{"location":"marketing/value-proposition/","title":"Why OTEL Tail Sampler?","text":""},{"location":"marketing/value-proposition/#the-problem-the-observability-data-explosion","title":"The Problem: The Observability Data Explosion","text":"<p>As systems scale, the cost of ingesting and storing telemetry data grows exponentially. Most organizations react by: 1. Head Sampling: Randomly dropping 90% of data at the source. This often drops the \"interesting\" traces (errors). 2. Ignoring Logs/Metrics: Reducing resolution to save costs, creating blind spots.</p>"},{"location":"marketing/value-proposition/#the-solution-intelligent-tail-sampling","title":"The Solution: Intelligent Tail Sampling","text":"<p>OTEL Tail Sampler allows you to move the sampling decision to the end of the trace. We buffer everything for a short period, and if a trace is deemed valuable, we flag it for export across your entire cluster.</p>"},{"location":"marketing/value-proposition/#business-impact","title":"\ud83d\udcb0 Business Impact","text":"<ul> <li>Reduced Cloud Spend: Lower your Datadog, Honeycomb, or Splunk bills by filtering out \"boring\" high-volume data before it ever leaves your network.</li> <li>Faster Incident Resolution: Mean Time to Resolution (MTTR) drops when every error trace is complete and contextually linked to relevant logs.</li> <li>Better Resource Utilization: Our Go-based, statically linked binaries run in minimal distroless containers, consuming minimal CPU and RAM.</li> </ul>"},{"location":"marketing/value-proposition/#use-cases","title":"\ud83c\udfd7 Use Cases","text":"<ul> <li>High-Volume APIs: Keep all 5xx error traces while sampling only 0.1% of successful 200 OK responses.</li> <li>E-commerce: Flag and keep all traces for \"VIP\" customers or high-value checkout attempts.</li> <li>Microservices: Use gossip-based coordination to ensure all spans of a distributed trace are kept, regardless of which node they landed on.</li> </ul>"}]}